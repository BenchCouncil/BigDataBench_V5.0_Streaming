%BigDataBench is an open-source comprehensive big data benchmark suites. There are 77 workloads in the latest version of BigDataBench --BigDataBench 3.0. Considering the diversity and representativeness, the workloads in BigDataBench cover different types of applications: cloud OLTP, OLAP and interactive analytics, and offline analytics. Moreover, each operation or algorithm has various implementations using different software stacks. In addition, these workloads cover both basic operations and state-of-art algorithms in three popular Internet scenarios: search engine, social network, and e-commerce.

%Since its release from June 2013, more than 20 research groups worldwide  have  published papers using BigDataBench. In short, the workloads in BigDataBench are manifold and comprehensive, so that they can meet the needs of users from different research fields, such as architecture, system, and networking. So in this section, for the comprehensiveness  of BigDataBench, we reduce it to produce representative big data workloads, however our methodology and tool can also apply to other big data benchmarks.

To reduce workload characterization difficulty and benchmarking cost of BigdataBench, we use the tool mentioned in Section \ref{section3} to reduce
the number of workloads from a perspective of  micro-architecture. As the input for the tool, we choose 45 micro architecture level metrics (the  specific metrics are show in \cite{jia_bigDataBench_subset}), covering the characteristics of \emph{Instruction Mix}, \emph{Cache Behavior}, \emph{Translation Look-aside Buffer (TLB) Behavior}, \emph{Branch Execution}, \emph{Pipeline Behavior}, \emph{Offcore Requests} and \emph{Snoop Responses}, \emph{Parallelism}, and \emph{Operation Intensity}.

When using the tool, we normalize these metric values to a Gaussian distribution and use Principle Component Analysis (PCA) to reduce the dimensions. Running the tool, we reduce 45 metrics to 9 principle components, the sum of whose eigenvalues accounts for 89.3 percentage of the sum of all eigenvalues. And then we use K-Means to cluster the 77 workloads, and there are 17 clusters in the final results.

Since the workloads in the same cluster are similar, we could choose one to represent the workloads in the cluster. There are two methods for choosing the representative workload from each cluster: 1) select the workload that is as close as possible to the center of the cluster it belongs to, and 2) select the extreme workload situated at the boundary of each cluster. We select the workload situated at the boundary of each cluster as the
representative workload. The rationale behind the approach would be that the behavior of the workloads in the middle of a cluster can be extracted from the behavior of the boundary.

The final 17 workloads reduced for architecture researchers are listed in Table~\ref{reduing workloads}.  In Table~\ref{reduing workloads}, we give a simple description of each representative big data workload, and describe each workload from the perspective of system behavior, data behaviors and application category.
\subsubsection{\textbf{System Behaviors}}

We choose CPU Usages, DISK IO behaviors and IO Bandwidth to analyze
the system behaviors of big data workloads. First, the CPU usages are
described by CPU utilization and IO Wait ratio. CPU utilization is defined as the percentage of time that the CPU executing at system or user level, while I/O Wait ratio is defined as the percentage of time that the CPU waiting for outstanding disk I/O requests. Second, DISK I/O performance is a key metric of big data workloads. We investigate the DISK I/O behavior, which are described by the weighted Disk I/O time ratio. Weighted Disk I/O time is defined as weighted number of milliseconds spent doing I/Os since the last update, and the average weighted Disk I/O time ratio is the weighted Disk I/O time divided by the running time of the workload. Third, we choose the Disk I/O Bandwidth, Network I/O Bandwidth and Memory Bandwidth to describe IO Bandwidth, which can reflect the I/O throughput requirements of big data workloads.

In order to better understand system behaviors, we roughly classify the workloads into three categories: (1)\emph{CPU-Intensive workloads},
which have high CPU utilization, low average weighted Disk I/O time ratio or I/O Bandwidth; for example, in our experiments, if the workload's CPU utilization is larger than 85\%, we consider it CPU-Intensive. (2)\emph{I/O-Intensive workloads}, which have high average weighted Disk I/O time ratio or I/O Bandwidth but low CPU utilization; such as, the workload's average weighted Disk I/O time ratio larger than 10 or the I/O Wait ratio larger than 20\%, and the CPU utilization less than 60\%, we consider it  I/O-Intensive; (3)\emph{Hybrid workloads}, whose behaviors are between CPU-Intensive workloads and IO-Intensive workloads. The three system behaviours categories of our 17 workloads are shown in Table \ref{reduing workloads}.

