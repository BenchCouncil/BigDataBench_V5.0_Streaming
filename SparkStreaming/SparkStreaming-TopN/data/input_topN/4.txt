the number of workloads from a perspective of  micro-architecture. As the input for the tool, we choose 45 micro architecture level metrics (the  specific metrics are show in \cite{jia_bigDataBench_subset}), covering the characteristics of \emph{Instruction Mix}, \emph{Cache Behavior}, \emph{Translation Look-aside Buffer (TLB) Behavior}, \emph{Branch Execution}, \emph{Pipeline Behavior}, \emph{Offcore Requests} and \emph{Snoop Responses}, \emph{Parallelism}, and \emph{Operation Intensity}.

When using the tool, we normalize these metric values to a Gaussian distribution and use Principle Component Analysis (PCA) to reduce the dimensions. Running the tool, we reduce 45 metrics to 9 principle components, the sum of whose eigenvalues accounts for 89.3 percentage of the sum of all eigenvalues. And then we use K-Means to cluster the 77 workloads, and there are 17 clusters in the final results.

Since the workloads in the same cluster are similar, we could choose one to represent the workloads in the cluster. There are two methods for choosing the representative workload from each cluster: 1) select the workload that is as close as possible to the center of the cluster it belongs to, and 2) select the extreme workload situated at the boundary of each cluster. We select the workload situated at the boundary of each cluster as the
representative workload. The rationale behind the approach would be that the behavior of the workloads in the middle of a cluster can be extracted from the behavior of the boundary.

The final 17 workloads reduced for architecture researchers are listed in Table~\ref{reduing workloads}.  In Table~\ref{reduing workloads}, we give a simple description of each representative big data workload, and describe each workload from the perspective of system behavior, data behaviors and application category.
