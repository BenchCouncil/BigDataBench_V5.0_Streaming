%After all workloads are finished, if the user only want to compare the performance data collected by profilers, then the figure plotter can be used to plot different kinds of graphes, such as histogram. The histogram facilitates users to compare the performance metrics among different workloads.

%Our tool also provides the function to show the similarity among workloads. The tool employs hierarchical clustering, which is one common way to do similarity analysis, for it can quantitatively show the similarity among workloads via a dendrogram.
%The dendrogram illustrates how each cluster is composed by drawing a U-shaped link between a non-singleton cluster and its children. The length of the top of the U-link is the distance between its children. The shorter the distance, the more similar between the children. Due to the space limitation, we do not provide details.


%Further,  use the single linkage distance to create the dendrogram.
\section{Representative big data Workloads}

%BigDataBench is an open-source comprehensive big data benchmark suites. There are 77 workloads in the latest version of BigDataBench --BigDataBench 3.0. Considering the diversity and representativeness, the workloads in BigDataBench cover different types of applications: cloud OLTP, OLAP and interactive analytics, and offline analytics. Moreover, each operation or algorithm has various implementations using different software stacks. In addition, these workloads cover both basic operations and state-of-art algorithms in three popular Internet scenarios: search engine, social network, and e-commerce.

%Since its release from June 2013, more than 20 research groups worldwide  have  published papers using BigDataBench. In short, the workloads in BigDataBench are manifold and comprehensive, so that they can meet the needs of users from different research fields, such as architecture, system, and networking. So in this section, for the comprehensiveness  of BigDataBench, we reduce it to produce representative big data workloads, however our methodology and tool can also apply to other big data benchmarks.

To reduce workload characterization difficulty and benchmarking cost of BigdataBench, we use the tool mentioned in Section \ref{section3} to reduce
the number of workloads from a perspective of  micro-architecture. As the input for the tool, we choose 45 micro architecture level metrics (the  specific metrics are show in \cite{jia_bigDataBench_subset}), covering the characteristics of \emph{Instruction Mix}, \emph{Cache Behavior}, \emph{Translation Look-aside Buffer (TLB) Behavior}, \emph{Branch Execution}, \emph{Pipeline Behavior}, \emph{Offcore Requests} and \emph{Snoop Responses}, \emph{Parallelism}, and \emph{Operation Intensity}.

When using the tool, we normalize these metric values to a Gaussian distribution and use Principle Component Analysis (PCA) to reduce the dimensions. Running the tool, we reduce 45 metrics to 9 principle components, the sum of whose eigenvalues accounts for 89.3 percentage of the sum of all eigenvalues. And then we use K-Means to cluster the 77 workloads, and there are 17 clusters in the final results.
